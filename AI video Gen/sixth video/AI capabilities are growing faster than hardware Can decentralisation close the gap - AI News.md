![](https://www.artificialintelligence-news.com/wp-content/uploads/2024/08/AI-capabilities.png)

### About the Author

By | August 20, 2024

![](https://www.artificialintelligence-news.com/wp-content/uploads/2024/08/ai-news.png)

AI capabilities have exploded over the past two years, with large language models (LLMs) such as ChatGPT, Dall-E, and Midjourney becoming everyday use tools. As you’re reading this article, generative AI programs are responding to emails, writing marketing copies, recording songs, and creating images from simple inputs. 

What’s even more remarkable to witness is the rate at which both individuals and companies are embracing the AI ecosystem. A recent [survey](https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai) by McKinsey revealed that the number of companies that have adopted generative AI in at least one business function doubled within a year to 65%, up from 33% at the beginning of 2023. 

However, like most technological advancements, this nascent area of innovation is not short of challenges. Training and running AI programs is resource intensive endeavour, and as things stand, big tech seems to have an upper hand which creates the risk of AI centralisation. 

## The computational limitation in AI development 

According to an [article](https://www.weforum.org/agenda/2024/04/how-to-manage-ais-energy-demand-today-tomorrow-and-in-the-future/) by the World Economic Forum, there is an accelerating demand for AI compute; the computational power required to sustain AI development is currently growing at an annual rate of between 26% and 36%.   

Another recent study by Epoch AI confirms this trajectory, with projections showing that it will soon cost billions of dollars to train or run AI programs. 

_“The cost of the largest AI training runs is growing by a factor of two to three per year since 2016, and that puts billion-dollar price tags on the horizon by 2027, maybe sooner,”_ [_noted_](https://time.com/6984292/cost-artificial-intelligence-compute-epoch-report/) _Epoch AI staff researcher, Ben Cottier._ 

In my opinion, we’re already at this point. Microsoft invested $10 billion in OpenAI last year and, more recently, news emerged that the two entities are planning to build a data center that will host a supercomputer powered by millions of specialised chips. The cost? A whopping $100 billion, which is ten times more than the initial investment. 

Well, Microsoft is not the only big tech that’s on a spending spree to boost its AI computing resources. Other companies in the AI arms race, including Google, Alphabet, and Nvidia are all directing a significant amount of funding to AI research and development. 

While we can agree that the outcome could match the amount of money being invested, it is hard to ignore the fact that AI development is currently a ‘big tech’ sport. Only these deep-pocketed companies have the ability to fund AI projects to the tune of tens or hundreds of billions. 

It begs the question; what can be done to avoid the same pitfalls that Web2 innovations are facing as a result of a handful of companies controlling innovation? 

Stanford’s HAI Vice Director and Faculty Director of Research, James Landay, is one of the experts who has previously [weighed](https://www.ibm.com/blog/artificial-intelligence-trends/) in on this scenario. According to Landay, the rush for GPU resources and the prioritisation by big tech companies to use their AI computational power in-house will trigger the demand for computing power, ultimately pushing stakeholders to develop cheaper hardware solutions.

In China, the government is already stepping up to support AI startups following the chip wars with the US that have limited Chinese companies from seamlessly accessing crucial chips. Local governments within China [introduced](https://www.ft.com/content/9d67cda3-b157-47a0-98cb-e8e9842b2c90) subsidies earlier this year, pledging to offer computing vouchers for AI startups ranging between $140,000 and $280,000. This effort is aimed at reducing the costs associated with computing power.

## Decentralising AI computing costs

Looking at the current state of AI computing, one theme is constant — the industry is currently centralised. Big tech companies control the majority of the computing power as well as AI programs. The more things change, the more they remain the same. 

On the brighter side, this time, things might actually change for good, thanks to decentralised computing infrastructures such as the [Qubic](https://qubic.org/#aigarth) Layer 1 blockchain. This L1 blockchain uses an advanced mining mechanism dubbed the useful Proof-of-Work (PoW); unlike Bitcoin’s typical PoW which uses energy for the sole purpose of securing the network, Qubic’s uPoW utilizes its computational power for productive AI tasks such as training neural networks. 

In simpler terms, Qubic is decentralising the sourcing of AI computational power by moving away from the current paradigm where innovators are limited to the hardware they own or have rented from big tech. Instead, this L1 is tapping into its network of miners which could run into the tens of thousands to provide computational power. 

Although a bit more technical than leaving big tech to handle the backend side of things, a decentralised approach to sourcing for AI computing power is more economical. But more importantly, it would only be fair if AI innovations would be driven by more stakeholders as opposed to the current state where the industry seems to rely on a few players. 

What happens if all of them go down? Make matters worse, these tech companies have proven untrustworthy with life-changing tech advancements. 

Today, most people are up in arms against data privacy violations, not to mention other affiliated issues such as societal manipulation. With decentralised AI innovations, it will be easier to check on the developments while reducing the cost of entry.  

## Conclusion 

AI innovations are just getting started, but the challenge of accessing computational power is still a headwind. To add to it, Big tech currently controls most of the resources which is a big challenge to the rate of innovation, not to mention the fact that these same companies could end up having more power over our data – the digital gold.  

However, with the advent of decentralised infrastructures, the entire AI ecosystem stands a better chance of reducing computational costs and eliminating big tech control over one of the most valuable technologies of the 21st century.

## OpenAI warns California’s AI bill threatens US innovation

![](https://www.artificialintelligence-news.com/wp-content/uploads/2024/08/openai-california-ai-safety-bill-legislation-ethics-law-letter.jpg)

![](https://secure.gravatar.com/avatar/b8c5d238e1fddd55d8a0064f1a534ba5?s=100&d=mm&r=g) Ryan Daws is a senior editor at TechForge Media with over a decade of experience in crafting compelling narratives and making complex topics accessible. His articles and interviews with industry leaders have earned him recognition as a key influencer by organisations like Onalytica. Under his leadership, publications have been praised by analyst firms such as Forrester for their excellence and performance. Connect with him on X (@gadget\_ry) or Mastodon (@gadgetry@techhub.social)

[OpenAI](https://openai.com/) has added its voice to the growing chorus of tech leaders and politicians opposing a controversial AI safety bill in California. The company argues that the legislation, [SB 1047](https://leginfo.legislature.ca.gov/faces/billNavClient.xhtml?bill_id=202320240SB1047), would stifle innovation and that regulation should be handled at a federal level.

In a letter sent to California State Senator Scott Wiener’s office, OpenAI expressed concerns that the bill could have “broad and significant” implications for US competitiveness and national security. The company argued that SB 1047 would threaten California’s position as a global leader in AI, prompting talent to seek “greater opportunity elsewhere.” 

Introduced by Senator Wiener, the bill aims to enact “common sense safety standards” for companies developing large AI models exceeding specific size and cost thresholds. These standards would require companies to implement shut-down mechanisms, take “reasonable care” to prevent catastrophic outcomes, and submit compliance statements to the California attorney general. Failure to comply could result in lawsuits and civil penalties.

Lieutenant General John (Jack) Shanahan, who served in the US Air Force and was the inaugural director of the US Department of Defense’s Joint Artificial Intelligence Center (JAIC), believes the bill “thoughtfully navigates the serious risks that AI poses to both civil society and national security” and provides “pragmatic solutions”.

Hon. Andrew C. Weber – former Assistant Secretary of Defense for Nuclear, Chemical, and Biological Defense Programs – echoed the national security concerns.

“The theft of a powerful AI system from a leading lab by our adversaries would impose considerable risks on us all,” said Weber. “Developers of the most advanced AI systems need to take significant cybersecurity precautions given the potential risks involved in their work. I’m glad to see that SB 1047 helps establish the necessary protective measures.”

SB 1047 has sparked fierce opposition from major tech companies, startups, and venture capitalists who argue that it overreaches for a nascent technology, potentially stifling innovation and driving businesses from the state. These concerns are echoed by OpenAI, with sources revealing that the company has paused plans to expand its San Francisco offices due to the uncertain regulatory landscape.

Senator Wiener [defended](https://sd11.senate.ca.gov/news/senator-wiener-responds-openai-opposition-sb-1047) the bill, stating that OpenAI’s letter fails to “criticise a single provision.” He dismissed concerns about talent exodus as “nonsensical,” stating that the law would apply to any company conducting business in California, regardless of their physical location. Wiener highlighted the bill’s “highly reasonable” requirement for large AI labs to test their models for catastrophic safety risks, a practice many have already committed to.

Critics, however, counter that mandating the submission of model details to the government will hinder innovation. They also fear that the threat of lawsuits will deter smaller, open-source developers from establishing startups.  In response to the backlash, Senator Wiener recently amended the bill to eliminate criminal liability for non-compliant companies, safeguard smaller developers, and remove the proposed “Frontier Model Division.”

OpenAI maintains that a clear federal framework, rather than state-level regulation, is essential for preserving public safety while maintaining  US competitiveness against rivals like China. The company highlighted the suitability of federal agencies, such as the White House Office of Science and Technology Policy and the Department of Commerce, to govern AI risks.

Senator Wiener acknowledged the ideal of congressional action but expressed scepticism about its likelihood. He drew parallels with California’s data privacy law, passed in the absence of federal action, suggesting that inaction from Congress shouldn’t preclude California from taking a leading role.

The California state assembly is set to vote on SB 1047 this month. If passed, the bill will land on the desk of Governor Gavin Newsom, whose stance on the legislation remains unclear. However, Newsom has publicly recognised the need to balance AI innovation with risk mitigation.

_(Photo by [Solen Feyissa](https://unsplash.com/@solenfeyissa?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash))_

**See also:** [**OpenAI delivers GPT-4o fine-tuning**](https://www.artificialintelligence-news.com/news/openai-delivers-gpt-4o-fine-tuning/)

[](https://www.ai-expo.net/)

**Want to learn more about AI and big data from industry leaders?** Check out [AI & Big Data Expo](https://www.ai-expo.net/) taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including [Intelligent Automation Conference](https://intelligentautomation-conference.com/northamerica/), [BlockX](https://www.blockchain-expo.com/), [Digital Transformation Week](https://digitaltransformation-week.com/), and [Cyber Security & Cloud Expo](https://www.cybersecuritycloudexpo.com/).

Explore other upcoming enterprise technology events and webinars powered by TechForge [here](https://techforge.pub/events/).

Tags: [ai](https://www.artificialintelligence-news.com/news/tag/ai/), [california](https://www.artificialintelligence-news.com/news/tag/california/), [government](https://www.artificialintelligence-news.com/news/tag/government/), [law](https://www.artificialintelligence-news.com/news/tag/law/), [legal](https://www.artificialintelligence-news.com/news/tag/legal/), [Legislation](https://www.artificialintelligence-news.com/news/tag/legislation/), [openai](https://www.artificialintelligence-news.com/news/tag/openai/), [Politics](https://www.artificialintelligence-news.com/news/tag/politics/), [sb 1047](https://www.artificialintelligence-news.com/news/tag/sb-1047/), [usa](https://www.artificialintelligence-news.com/news/tag/usa/)